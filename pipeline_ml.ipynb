{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ================================================\n",
    "# PIPELINE DE MACHINE LEARNING COM PLN E VECTORDB\n",
    "# ================================================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INSTALAÇÃO DE DEPENDÊNCIAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install sentence-transformers\n",
    "# !pip install chromadb\n",
    "# !pip install pdfplumber\n",
    "# !pip install nltk\n",
    "# !pip install spacy\n",
    "# !python -m spacy download pt_core_news_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "import re\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_text_splitters import MarkdownHeaderTextSplitter\n",
    "\n",
    "#nltk.download('stopwords') # rodar apenas uma vez"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ETAPAS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. CARREGAMENTO DE ARQUIVOS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definição das funções"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def carregar_pdf(caminho_arquivo):\n",
    "    \"\"\"Carrega e extrai texto de um arquivo PDF\"\"\"\n",
    "    try:\n",
    "        import pdfplumber\n",
    "        texto_completo = []\n",
    "        with pdfplumber.open(caminho_arquivo) as pdf:\n",
    "            for pagina in pdf.pages:\n",
    "                texto = pagina.extract_text()\n",
    "                if texto:\n",
    "                    texto_completo.append(texto)\n",
    "        return \"\\n\".join(texto_completo)\n",
    "    except ImportError:\n",
    "        print(\"⚠ pdfplumber não disponível. Instale com: pip install pdfplumber\")\n",
    "        return \"\"\n",
    "\n",
    "def carregar_csv(caminho_arquivo):\n",
    "    \"\"\"Carrega e extrai texto de um arquivo CSV\"\"\"\n",
    "    textos = []\n",
    "    with open(caminho_arquivo, 'r', encoding='utf-8') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for linha in reader:\n",
    "            texto = \" \".join([str(v) for v in linha.values()])\n",
    "            textos.append(texto)\n",
    "    return \"\\n\".join(textos)\n",
    "\n",
    "def carregar_json(caminho_arquivo):\n",
    "    \"\"\"Carrega e extrai texto de um arquivo JSON\"\"\"\n",
    "    with open(caminho_arquivo, 'r', encoding='utf-8') as f:\n",
    "        dados = json.load(f)\n",
    "\n",
    "    textos = []\n",
    "\n",
    "    def extrair_texto_recursivo(obj):\n",
    "        if isinstance(obj, dict):\n",
    "            for valor in obj.values():\n",
    "                extrair_texto_recursivo(valor)\n",
    "        elif isinstance(obj, list):\n",
    "            for item in obj:\n",
    "                extrair_texto_recursivo(item)\n",
    "        elif isinstance(obj, str):\n",
    "            textos.append(obj)\n",
    "\n",
    "    extrair_texto_recursivo(dados)\n",
    "    return \"\\n\".join(textos)\n",
    "\n",
    "def carregar_txt(caminho_arquivo):\n",
    "    \"\"\"Carrega e extrai texto de um arquivo TXT\"\"\"\n",
    "    with open(caminho_arquivo, 'r', encoding='utf-8') as f:\n",
    "        return f.read()\n",
    "\n",
    "def carregar_arquivo(caminho_arquivo):\n",
    "    \"\"\"Função universal para carregar arquivos PDF, CSV ou JSON\"\"\"\n",
    "    extensao = caminho_arquivo.split('.')[-1].lower()\n",
    "\n",
    "    if extensao == 'pdf':\n",
    "        return carregar_pdf(caminho_arquivo)\n",
    "    elif extensao == 'csv':\n",
    "        return carregar_csv(caminho_arquivo)\n",
    "    elif extensao == 'json':\n",
    "        return carregar_json(caminho_arquivo)\n",
    "    elif extensao == 'txt':\n",
    "        return carregar_txt(caminho_arquivo)\n",
    "    else:\n",
    "        raise ValueError(f\"Formato não suportado: {extensao}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Execução"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Título: Introdução ao Machine Learning\n",
      "Machine learning é um campo da inteligência artificial que desenvolve algoritmos\n",
      "capazes de aprender padrões a partir de dados. Os principais tipos incluem\n",
      "aprendizado supervisionado, aprendizado não supervisionado e aprendizado por\n",
      "reforço.\n",
      "Aprendizado supervisionado envolve treinar um modelo com dados rotulados,\n",
      "como prever preços de casas com base em características como tamanho,\n",
      "localização e número de quartos. Aprendizado não supervisionado detecta\n",
      "padrões ocultos em dados não rotulados, como segmentação de clientes em\n",
      "marketing. Aprendizado por reforço ensina agentes a tomar decisões em\n",
      "ambientes dinâmicos para maximizar recompensas.\n",
      "Redes neurais profundas são usadas em visão computacional e processamento\n",
      "de linguagem natural. Modelos como CNNs e Transformers têm revolucionado\n",
      "tarefas como tradução automática, reconhecimento de imagens e geração de\n",
      "texto.\n"
     ]
    }
   ],
   "source": [
    "texto = \"\"\n",
    "texto = carregar_arquivo('introducaoml.pdf')\n",
    "print(texto)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. PROCESSAMENTO DE LINGUAGEM NATURAL (PLN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definição das Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('pt_core_news_sm')\n",
    "\n",
    "API_STOPWORDS_PT = set(stopwords.words('portuguese'))\n",
    "API_STOPWORDS_PT.remove('não')  # Mantendo a palavra \"não\"\n",
    "\n",
    "STOPWORDS_EN = {'a', 'an', 'and', 'are', 'as', 'at', 'be', 'by', 'for', 'from', \n",
    "                'has', 'he', 'in', 'is', 'it', 'its', 'of', 'on', 'that', 'the', \n",
    "                'to', 'was', 'will', 'with', 'this', 'but', 'they', 'have', 'had'}\n",
    "\n",
    "# Carregando apenas as stopwords em português\n",
    "\n",
    "STOPWORDS_ALL = API_STOPWORDS_PT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pré-Processamento de Texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def limpar_texto(texto):\n",
    "    \"\"\"Realiza limpeza básica do texto\"\"\"\n",
    "    texto = texto.lower()\n",
    "    texto = re.sub(r'\\n\\n', ' \\n\\n', texto) # manter quebras de parágrafo em caso de recursive chunking\n",
    "    texto = re.sub(r'\\n', ' \\n', texto)\n",
    "    texto = re.sub(r'\\.', ' .', texto)\n",
    "    texto = re.sub(r'[^a-z0-9áàâãéêíóôõúüçñ\\s\\n.]', ' ', texto) # ^ indica negação\n",
    "    texto = re.sub(r'  +', ' ', texto) # multiplos espaços\n",
    "    return texto.strip()\n",
    "\n",
    "# Substituir acentos em caso de stemização\n",
    "\n",
    "def tokenizar_simples(texto):\n",
    "    \"\"\"Tokeniza o texto (divisão por espaços)\"\"\"\n",
    "    return texto.split()\n",
    "\n",
    "def tokenizar_spacy(texto):\n",
    "    \"\"\"Tokeniza o texto usando spaCy\"\"\"\n",
    "    doc = nlp(texto)\n",
    "    return [token for token in doc]\n",
    "\n",
    "def remover_stopwords(tokens, stopwords=STOPWORDS_ALL):\n",
    "    \"\"\"Remove stopwords em português e inglês\"\"\"\n",
    "    return [token for token in tokens if token.text not in stopwords]\n",
    "\n",
    "def lematizar_texto(tokens):\n",
    "    \"\"\"Pega a palavra lema de cada token no texto\"\"\"\n",
    "    return [token.lemma_ for token in tokens]\n",
    "\n",
    "def processar_texto_pln(texto):\n",
    "    \"\"\"Pipeline completo de PLN\"\"\"\n",
    "    texto_limpo = limpar_texto(texto)\n",
    "    palavras = tokenizar_spacy(texto_limpo)\n",
    "    palavras_sem_stopwords = remover_stopwords(palavras)\n",
    "    palavras_lemma = lematizar_texto(palavras_sem_stopwords)\n",
    "    return ' '.join(palavras_lemma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Execução"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "título introdução machine learning \n",
      " machine learning campo inteligência artificial desenvolver algorit \n",
      " capaz aprender padrão partir dado . principal tipo incluir \n",
      " aprendizar supervisionar aprendizar não supervisionar aprendizar \n",
      " reforço . \n",
      " aprendizar supervisionar envolver treinar modelo dado rotular \n",
      " prever preço casa base característica tamanho \n",
      " localização número quarto . aprendizar não supervisionar detectar \n",
      " padrão oculto dado não rotular segmentação cliente \n",
      " marketing . aprendizar reforço ensina agente tomar decisão \n",
      " ambiente dinâmico maximizar recompensa . \n",
      " rede neural profundo usar visão computacional processamento \n",
      " linguagem natural . modelo cnns transformer ter revolucionar \n",
      " tarefa tradução automático reconhecimento imagem geração \n",
      " texto .\n"
     ]
    }
   ],
   "source": [
    "texto_processado = processar_texto_pln(texto)\n",
    "print(texto_processado)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. CHUNKING COM OVERLAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tipos relevantes de chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fixed-Length Chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Divide o texto em pedaços com número fixo de caracteres ou tokens. É simples de implementar e previsível, porém pode cortar informações importantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_chunks:  7\n",
      "['título introdução machine learning \\n machine learning campo inteligência artificial desenvolver algorit \\n capaz aprender padrão partir dado', 'principal tipo incluir \\n aprendizar supervisionar aprendizar não supervisionar aprendizar \\n reforço', 'aprendizar supervisionar envolver treinar modelo dado rotular \\n prever preço casa base característica tamanho \\n localização número quarto', 'aprendizar não supervisionar detectar \\n padrão oculto dado não rotular segmentação cliente \\n marketing', 'aprendizar reforço ensina agente tomar decisão \\n ambiente dinâmico maximizar recompensa', 'rede neural profundo usar visão computacional processamento \\n linguagem natural', 'modelo cnns transformer ter revolucionar \\n tarefa tradução automático reconhecimento imagem geração \\n texto']\n"
     ]
    }
   ],
   "source": [
    "# EXEMPLO:\n",
    "\n",
    "fixed_splitter = CharacterTextSplitter(\n",
    "    separator=\".\",\n",
    "    chunk_size=150,\n",
    "    chunk_overlap=20\n",
    ")\n",
    "\n",
    "chunks = fixed_splitter.split_text(texto_processado)\n",
    "print(\"n_chunks: \", len(chunks))\n",
    "print(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Recursive Character Chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Divide o texto progressivamente, mantendo mantendo estruturas naturais (parágrafos, frases), a melhor abordagem na maioria dos casos, superior ao fixed-length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_chunks:  7\n",
      "['título introdução machine learning \\n machine learning campo inteligência artificial desenvolver algorit \\n capaz aprender padrão partir dado', '. principal tipo incluir \\n aprendizar supervisionar aprendizar não supervisionar aprendizar \\n reforço', '. \\n aprendizar supervisionar envolver treinar modelo dado rotular \\n prever preço casa base característica tamanho \\n localização número quarto', '. aprendizar não supervisionar detectar \\n padrão oculto dado não rotular segmentação cliente \\n marketing', '. aprendizar reforço ensina agente tomar decisão \\n ambiente dinâmico maximizar recompensa', '. \\n rede neural profundo usar visão computacional processamento \\n linguagem natural', '. modelo cnns transformer ter revolucionar \\n tarefa tradução automático reconhecimento imagem geração \\n texto .']\n"
     ]
    }
   ],
   "source": [
    "# EXEMPLO:\n",
    "\n",
    "recursive_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=150,\n",
    "    chunk_overlap=20,\n",
    "    separators=[\"\\n\\n\", \".\", \"\\n\", \" \", \"\"] # '.' tem mais valor semântico que '\\n' pelo modo como o arquivo(pdf) é carregado\n",
    ")\n",
    "\n",
    "chunks = recursive_splitter.split_text(texto_processado)\n",
    "print(\"n_chunks: \", len(chunks))\n",
    "print(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Document-Based Chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Divide o texto respeitando a estrutura hierárquica do documento, ótima abordagem para casos envolvendo textos com markdown, artigos científicos e acadêmicos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXEMPLO COM MARKDOWN:\n",
    "\n",
    "headers_to_split_on = [\n",
    "    (\"#\", \"Header 1\"),\n",
    "    (\"##\", \"Header 2\"),\n",
    "    (\"###\", \"Header 3\"),\n",
    "]\n",
    "\n",
    "markdown_splitter = MarkdownHeaderTextSplitter(\n",
    "    headers_to_split_on=headers_to_split_on\n",
    ")\n",
    "\n",
    "chunks_document = markdown_splitter.split_text(documento_markdown)\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=150,\n",
    "    chunk_overlap=20\n",
    ")\n",
    "\n",
    "final_chunks = text_splitter.split_documents(chunks_document)\n",
    "\n",
    "# chunks_document\n",
    "# metadata definidos no headers_to_split_on -> chunk.metadata # {'Header 1': 'Introdução ao PLN'}\n",
    "# conteúdo  # Texto ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. EMBEDDING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def criar_embeddings(chunks, usar_modelo_real=True):\n",
    "    \"\"\"Cria embeddings para os chunks\"\"\"\n",
    "    if usar_modelo_real:\n",
    "        try:\n",
    "            from sentence_transformers import SentenceTransformer\n",
    "\n",
    "            print(\"Carregando modelo de embedding...\")\n",
    "            modelo = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n",
    "            print(\"✓ Modelo carregado\")\n",
    "\n",
    "            embeddings = modelo.encode(chunks, show_progress_bar=True)\n",
    "            return embeddings\n",
    "\n",
    "        except ImportError:\n",
    "            print(\"⚠ sentence-transformers não disponível\")\n",
    "            print(\"Usando embeddings simulados...\")\n",
    "            usar_modelo_real = False\n",
    "\n",
    "    if not usar_modelo_real:\n",
    "        # Embeddings simulados para demonstração\n",
    "        dimensao = 384\n",
    "        embeddings = np.random.randn(len(chunks), dimensao).astype('float32')\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. VECTOR DATABASE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorDB:\n",
    "    \"\"\"Banco de dados vetorial simples\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.embeddings = []\n",
    "        self.documentos = []\n",
    "        self.metadados = []\n",
    "        self.ids = []\n",
    "\n",
    "    def adicionar(self, embeddings, documentos, metadados, ids):\n",
    "        \"\"\"Adiciona documentos ao banco\"\"\"\n",
    "        self.embeddings = np.array(embeddings)\n",
    "        self.documentos = documentos\n",
    "        self.metadados = metadados\n",
    "        self.ids = ids\n",
    "\n",
    "    def buscar(self, query_embedding, n_resultados=3):\n",
    "        \"\"\"Busca documentos similares (similaridade de cosseno)\"\"\"\n",
    "        query_norm = query_embedding / np.linalg.norm(query_embedding)\n",
    "        docs_norm = self.embeddings / np.linalg.norm(self.embeddings, axis=1, keepdims=True)\n",
    "\n",
    "        similaridades = np.dot(docs_norm, query_norm)\n",
    "        indices_top = np.argsort(similaridades)[::-1][:n_resultados]\n",
    "\n",
    "        resultados = []\n",
    "        for idx in indices_top:\n",
    "            resultados.append({\n",
    "                'id': self.ids[idx],\n",
    "                'documento': self.documentos[idx],\n",
    "                'metadados': self.metadados[idx],\n",
    "                'similaridade': float(similaridades[idx])\n",
    "            })\n",
    "\n",
    "        return resultados\n",
    "\n",
    "    def salvar(self, caminho='vectordb.pkl'):\n",
    "        \"\"\"Salva o banco em arquivo\"\"\"\n",
    "        dados = {\n",
    "            'embeddings': self.embeddings,\n",
    "            'documentos': self.documentos,\n",
    "            'metadados': self.metadados,\n",
    "            'ids': self.ids\n",
    "        }\n",
    "        with open(caminho, 'wb') as f:\n",
    "            pickle.dump(dados, f)\n",
    "        print(f\"✓ Banco salvo em: {caminho}\")\n",
    "\n",
    "    def carregar(self, caminho='vectordb.pkl'):\n",
    "        \"\"\"Carrega o banco de arquivo\"\"\"\n",
    "        with open(caminho, 'rb') as f:\n",
    "            dados = pickle.load(f)\n",
    "        self.embeddings = dados['embeddings']\n",
    "        self.documentos = dados['documentos']\n",
    "        self.metadados = dados['metadados']\n",
    "        self.ids = dados['ids']\n",
    "        print(f\"✓ Banco carregado de: {caminho}\")\n",
    "\n",
    "    def contar(self):\n",
    "        \"\"\"Retorna número de documentos\"\"\"\n",
    "        return len(self.documentos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. BUSCA VETORIAL (QUERY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def realizar_busca_vetorial(vector_db, query, processar_query=True, n_resultados=3):\n",
    "    \"\"\"\n",
    "    Realiza busca vetorial\n",
    "\n",
    "    Args:\n",
    "        vector_db: Instância do banco de dados vetorial\n",
    "        query: Texto da query\n",
    "        processar_query: Se True, aplica PLN na query\n",
    "        n_resultados: Número de resultados a retornar\n",
    "    \"\"\"\n",
    "    print(f\"\\nQUERY: {query}\")\n",
    "    print(\"-\" * 70)\n",
    "\n",
    "    # Processar query (mesmo pipeline do texto)\n",
    "    if processar_query:\n",
    "        query_processada = processar_texto_pln(query)\n",
    "        print(f\"Query processada: {query_processada}\")\n",
    "    else:\n",
    "        query_processada = query\n",
    "\n",
    "    # Criar embedding da query\n",
    "    # Nota: Em produção, usar o mesmo modelo dos documentos\n",
    "    query_embedding = criar_embeddings([query_processada], usar_modelo_real=False)[0]\n",
    "\n",
    "    # Buscar documentos similares\n",
    "    resultados = vector_db.buscar(query_embedding, n_resultados=n_resultados)\n",
    "\n",
    "    print(f\"\\nResultados: {len(resultados)}\")\n",
    "    for i, resultado in enumerate(resultados, 1):\n",
    "        print(f\"\\n[{i}] ID: {resultado['id']}\")\n",
    "        print(f\"    Similaridade: {resultado['similaridade']:.4f}\")\n",
    "        print(f\"    Documento: {resultado['documento'][:150]}...\")\n",
    "\n",
    "    return resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "EXEMPLO DE USO DO PIPELINE\n",
      "================================================================================\n",
      "\n",
      "✓ Texto processado: 433 caracteres\n",
      "✓ Criados 1 chunks\n",
      "✓ Embeddings criados: (1, 384)\n",
      "✓ Banco vetorial criado com 1 documentos\n",
      "✓ Banco salvo em: meu_vectordb.pkl\n",
      "\n",
      "QUERY: machine learning\n",
      "----------------------------------------------------------------------\n",
      "Query processada: machine learning\n",
      "\n",
      "Resultados: 1\n",
      "\n",
      "[1] ID: chunk_0\n",
      "    Similaridade: -0.0554\n",
      "    Documento: documento teste machine learning embeddings representacao vetori embeddings sao representacoes numericas dados espaco vetori alta dimensao word embedd...\n",
      "\n",
      "QUERY: processamento de linguagem natural\n",
      "----------------------------------------------------------------------\n",
      "Query processada: processamento linguagem natur\n",
      "\n",
      "Resultados: 1\n",
      "\n",
      "[1] ID: chunk_0\n",
      "    Similaridade: -0.0262\n",
      "    Documento: documento teste machine learning embeddings representacao vetori embeddings sao representacoes numericas dados espaco vetori alta dimensao word embedd...\n",
      "\n",
      "QUERY: inteligência artificial\n",
      "----------------------------------------------------------------------\n",
      "Query processada: intelig artifici\n",
      "\n",
      "Resultados: 1\n",
      "\n",
      "[1] ID: chunk_0\n",
      "    Similaridade: -0.0151\n",
      "    Documento: documento teste machine learning embeddings representacao vetori embeddings sao representacoes numericas dados espaco vetori alta dimensao word embedd...\n",
      "\n",
      "================================================================================\n",
      "PIPELINE CONCLUÍDO COM SUCESSO!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EXEMPLO DE USO DO PIPELINE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 1. Carregar arquivos\n",
    "texto = carregar_arquivo('indroducaoml.pdf')\n",
    "#texto = \"Seu texto de exemplo aqui...\"\n",
    "\n",
    "# 2. Processar com PLN\n",
    "texto_processado = processar_texto_pln(texto)\n",
    "print(f\"\\n✓ Texto processado: {len(texto_processado)} caracteres\")\n",
    "\n",
    "# 3. Criar chunks\n",
    "chunks = criar_chunks(texto_processado, tamanho_chunk=50, overlap=5)\n",
    "print(f\"✓ Criados {len(chunks)} chunks\")\n",
    "\n",
    "# 4. Criar embeddings\n",
    "embeddings = criar_embeddings(chunks, usar_modelo_real=False)\n",
    "print(f\"✓ Embeddings criados: {embeddings.shape}\")\n",
    "\n",
    "# 5. Criar e popular banco vetorial\n",
    "vector_db = VectorDB()\n",
    "ids = [f\"chunk_{i}\" for i in range(len(chunks))]\n",
    "metadados = [{\"index\": i} for i in range(len(chunks))]\n",
    "vector_db.adicionar(embeddings, chunks, metadados, ids)\n",
    "print(f\"✓ Banco vetorial criado com {vector_db.contar()} documentos\")\n",
    "\n",
    "# 6. Salvar banco\n",
    "vector_db.salvar('meu_vectordb.pkl')\n",
    "\n",
    "# 7. Realizar buscas\n",
    "queries = [\n",
    "    \"machine learning\",\n",
    "    \"processamento de linguagem natural\",\n",
    "    \"inteligência artificial\"\n",
    "]\n",
    "\n",
    "for query in queries:\n",
    "    realizar_busca_vetorial(vector_db, query, n_resultados=2)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PIPELINE CONCLUÍDO COM SUCESSO!\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
