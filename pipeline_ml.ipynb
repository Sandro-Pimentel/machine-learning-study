{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ================================================\n",
    "# PIPELINE DE MACHINE LEARNING COM PLN E VECTORDB\n",
    "# ================================================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INSTALAÇÃO DE DEPENDÊNCIAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install sentence-transformers\n",
    "# !pip install chromadb\n",
    "# !pip install pdfplumber\n",
    "# !pip install nltk\n",
    "# !pip install spacy\n",
    "# !python -m spacy download pt_core_news_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "import re\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import spacy\n",
    "import nltk\n",
    "import chromadb\n",
    "from nltk.corpus import stopwords\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_text_splitters import MarkdownHeaderTextSplitter\n",
    "\n",
    "#nltk.download('stopwords') # rodar apenas uma vez"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ETAPAS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. CARREGAMENTO DE ARQUIVOS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definição das funções"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def carregar_pdf(caminho_arquivo):\n",
    "    \"\"\"Carrega e extrai texto de um arquivo PDF\"\"\"\n",
    "    try:\n",
    "        import pdfplumber\n",
    "        texto_completo = []\n",
    "        with pdfplumber.open(caminho_arquivo) as pdf:\n",
    "            for pagina in pdf.pages:\n",
    "                texto = pagina.extract_text()\n",
    "                if texto:\n",
    "                    texto_completo.append(texto)\n",
    "        return \"\\n\".join(texto_completo)\n",
    "    except ImportError:\n",
    "        print(\"⚠ pdfplumber não disponível. Instale com: pip install pdfplumber\")\n",
    "        return \"\"\n",
    "\n",
    "def carregar_csv(caminho_arquivo):\n",
    "    \"\"\"Carrega e extrai texto de um arquivo CSV\"\"\"\n",
    "    textos = []\n",
    "    with open(caminho_arquivo, 'r', encoding='utf-8') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for linha in reader:\n",
    "            texto = \" \".join([str(v) for v in linha.values()])\n",
    "            textos.append(texto)\n",
    "    return \"\\n\".join(textos)\n",
    "\n",
    "def carregar_json(caminho_arquivo):\n",
    "    \"\"\"Carrega e extrai texto de um arquivo JSON\"\"\"\n",
    "    with open(caminho_arquivo, 'r', encoding='utf-8') as f:\n",
    "        dados = json.load(f)\n",
    "\n",
    "    textos = []\n",
    "\n",
    "    def extrair_texto_recursivo(obj):\n",
    "        if isinstance(obj, dict):\n",
    "            for valor in obj.values():\n",
    "                extrair_texto_recursivo(valor)\n",
    "        elif isinstance(obj, list):\n",
    "            for item in obj:\n",
    "                extrair_texto_recursivo(item)\n",
    "        elif isinstance(obj, str):\n",
    "            textos.append(obj)\n",
    "\n",
    "    extrair_texto_recursivo(dados)\n",
    "    return \"\\n\".join(textos)\n",
    "\n",
    "def carregar_txt(caminho_arquivo):\n",
    "    \"\"\"Carrega e extrai texto de um arquivo TXT\"\"\"\n",
    "    with open(caminho_arquivo, 'r', encoding='utf-8') as f:\n",
    "        return f.read()\n",
    "\n",
    "def carregar_arquivo(caminho_arquivo):\n",
    "    \"\"\"Função universal para carregar arquivos PDF, CSV ou JSON\"\"\"\n",
    "    extensao = caminho_arquivo.split('.')[-1].lower()\n",
    "\n",
    "    if extensao == 'pdf':\n",
    "        return carregar_pdf(caminho_arquivo)\n",
    "    elif extensao == 'csv':\n",
    "        return carregar_csv(caminho_arquivo)\n",
    "    elif extensao == 'json':\n",
    "        return carregar_json(caminho_arquivo)\n",
    "    elif extensao == 'txt':\n",
    "        return carregar_txt(caminho_arquivo)\n",
    "    else:\n",
    "        raise ValueError(f\"Formato não suportado: {extensao}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Execução"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Título: Introdução ao Machine Learning\n",
      "Machine learning é um campo da inteligência artificial que desenvolve algoritmos\n",
      "capazes de aprender padrões a partir de dados. Os principais tipos incluem\n",
      "aprendizado supervisionado, aprendizado não supervisionado e aprendizado por\n",
      "reforço.\n",
      "Aprendizado supervisionado envolve treinar um modelo com dados rotulados,\n",
      "como prever preços de casas com base em características como tamanho,\n",
      "localização e número de quartos. Aprendizado não supervisionado detecta\n",
      "padrões ocultos em dados não rotulados, como segmentação de clientes em\n",
      "marketing. Aprendizado por reforço ensina agentes a tomar decisões em\n",
      "ambientes dinâmicos para maximizar recompensas.\n",
      "Redes neurais profundas são usadas em visão computacional e processamento\n",
      "de linguagem natural. Modelos como CNNs e Transformers têm revolucionado\n",
      "tarefas como tradução automática, reconhecimento de imagens e geração de\n",
      "texto.\n"
     ]
    }
   ],
   "source": [
    "texto = \"\"\n",
    "texto = carregar_arquivo('introducaoml.pdf')\n",
    "print(texto)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. PROCESSAMENTO DE LINGUAGEM NATURAL (PLN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definição das Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('pt_core_news_sm')\n",
    "\n",
    "API_STOPWORDS_PT = set(stopwords.words('portuguese'))\n",
    "API_STOPWORDS_PT.remove('não')  # Mantendo a palavra \"não\"\n",
    "\n",
    "STOPWORDS_EN = {'a', 'an', 'and', 'are', 'as', 'at', 'be', 'by', 'for', 'from', \n",
    "                'has', 'he', 'in', 'is', 'it', 'its', 'of', 'on', 'that', 'the', \n",
    "                'to', 'was', 'will', 'with', 'this', 'but', 'they', 'have', 'had'}\n",
    "\n",
    "# Carregando apenas as stopwords em português\n",
    "\n",
    "STOPWORDS_ALL = API_STOPWORDS_PT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pré-Processamento de Texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def limpar_texto(texto):\n",
    "    \"\"\"Realiza limpeza básica do texto\"\"\"\n",
    "    texto = texto.lower()\n",
    "    texto = re.sub(r'\\n\\n', ' \\n\\n', texto) # manter quebras de parágrafo em caso de recursive chunking\n",
    "    texto = re.sub(r'\\n', ' \\n', texto)\n",
    "    texto = re.sub(r'\\.', ' .', texto)\n",
    "    texto = re.sub(r'[^a-z0-9áàâãéêíóôõúüçñ\\s\\n.]', ' ', texto) # ^ indica negação\n",
    "    texto = re.sub(r'  +', ' ', texto) # multiplos espaços\n",
    "    return texto.strip()\n",
    "\n",
    "# Substituir acentos em caso de stemização\n",
    "\n",
    "def tokenizar_simples(texto):\n",
    "    \"\"\"Tokeniza o texto (divisão por espaços)\"\"\"\n",
    "    return texto.split()\n",
    "\n",
    "def tokenizar_spacy(texto):\n",
    "    \"\"\"Tokeniza o texto usando spaCy\"\"\"\n",
    "    doc = nlp(texto)\n",
    "    return [token for token in doc]\n",
    "\n",
    "def remover_stopwords(tokens, stopwords=STOPWORDS_ALL):\n",
    "    \"\"\"Remove stopwords em português e inglês\"\"\"\n",
    "    return [token for token in tokens if token.text not in stopwords]\n",
    "\n",
    "def lematizar_texto(tokens):\n",
    "    \"\"\"Pega a palavra lema de cada token no texto\"\"\"\n",
    "    return [token.lemma_ for token in tokens]\n",
    "\n",
    "def processar_texto_pln(texto):\n",
    "    \"\"\"Pipeline completo de PLN\"\"\"\n",
    "    texto_limpo = limpar_texto(texto)\n",
    "    palavras = tokenizar_spacy(texto_limpo)\n",
    "    palavras_sem_stopwords = remover_stopwords(palavras)\n",
    "    palavras_lemma = lematizar_texto(palavras_sem_stopwords)\n",
    "    return ' '.join(palavras_lemma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Execução"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "título introdução machine learning \n",
      " machine learning campo inteligência artificial desenvolver algorit \n",
      " capaz aprender padrão partir dado . principal tipo incluir \n",
      " aprendizar supervisionar aprendizar não supervisionar aprendizar \n",
      " reforço . \n",
      " aprendizar supervisionar envolver treinar modelo dado rotular \n",
      " prever preço casa base característica tamanho \n",
      " localização número quarto . aprendizar não supervisionar detectar \n",
      " padrão oculto dado não rotular segmentação cliente \n",
      " marketing . aprendizar reforço ensina agente tomar decisão \n",
      " ambiente dinâmico maximizar recompensa . \n",
      " rede neural profundo usar visão computacional processamento \n",
      " linguagem natural . modelo cnns transformer ter revolucionar \n",
      " tarefa tradução automático reconhecimento imagem geração \n",
      " texto .\n"
     ]
    }
   ],
   "source": [
    "texto_processado = processar_texto_pln(texto)\n",
    "print(texto_processado)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. CHUNKING COM OVERLAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tipos relevantes de chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fixed-Length Chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Divide o texto em pedaços com número fixo de caracteres ou tokens. É simples de implementar e previsível, porém pode cortar informações importantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_chunks:  7\n",
      "['título introdução machine learning \\n machine learning campo inteligência artificial desenvolver algorit \\n capaz aprender padrão partir dado', 'principal tipo incluir \\n aprendizar supervisionar aprendizar não supervisionar aprendizar \\n reforço', 'aprendizar supervisionar envolver treinar modelo dado rotular \\n prever preço casa base característica tamanho \\n localização número quarto', 'aprendizar não supervisionar detectar \\n padrão oculto dado não rotular segmentação cliente \\n marketing', 'aprendizar reforço ensina agente tomar decisão \\n ambiente dinâmico maximizar recompensa', 'rede neural profundo usar visão computacional processamento \\n linguagem natural', 'modelo cnns transformer ter revolucionar \\n tarefa tradução automático reconhecimento imagem geração \\n texto']\n"
     ]
    }
   ],
   "source": [
    "# EXEMPLO:\n",
    "\n",
    "fixed_splitter = CharacterTextSplitter(\n",
    "    separator=\".\",\n",
    "    chunk_size=150,\n",
    "    chunk_overlap=20\n",
    ")\n",
    "\n",
    "chunks = fixed_splitter.split_text(texto_processado)\n",
    "print(\"n_chunks: \", len(chunks))\n",
    "print(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Recursive Character Chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Divide o texto progressivamente, mantendo mantendo estruturas naturais (parágrafos, frases), a melhor abordagem na maioria dos casos, superior ao fixed-length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_chunks:  7\n",
      "['título introdução machine learning machine learning campo inteligência artificial desenvolver algorit capaz aprender padrão partir dado', '. principal tipo incluir aprendizar supervisionar aprendizar não supervisionar aprendizar reforço', '. aprendizar supervisionar envolver treinar modelo dado rotular prever preço casa base característica tamanho localização número quarto', '. aprendizar não supervisionar detectar padrão oculto dado não rotular segmentação cliente marketing', '. aprendizar reforço ensina agente tomar decisão ambiente dinâmico maximizar recompensa', '. rede neural profundo usar visão computacional processamento linguagem natural', '. modelo cnns transformer ter revolucionar tarefa tradução automático reconhecimento imagem geração texto']\n"
     ]
    }
   ],
   "source": [
    "# EXEMPLO:\n",
    "\n",
    "recursive_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=150,\n",
    "    chunk_overlap=20,\n",
    "    separators=[\"\\n\\n\", \".\", \"\\n\", \" \", \"\"] # '.' tem mais valor semântico que '\\n' pelo modo como o arquivo(pdf) é carregado\n",
    ")\n",
    "\n",
    "chunks = recursive_splitter.split_text(texto_processado)\n",
    "\n",
    "for index, chunk in enumerate(chunks):\n",
    "    chunks[index] = re.sub(r' \\n', '', chunk)\n",
    "    chunks[index] = re.sub(r' \\.', '', chunks[index])\n",
    "    #chunk = re.sub(r'\\,', '', chunk)\n",
    "\n",
    "print(\"n_chunks: \", len(chunks))\n",
    "print(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Document-Based Chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Divide o texto respeitando a estrutura hierárquica do documento, ótima abordagem para casos envolvendo textos com markdown, artigos científicos e acadêmicos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXEMPLO COM MARKDOWN:\n",
    "\n",
    "headers_to_split_on = [\n",
    "    (\"#\", \"Header 1\"),\n",
    "    (\"##\", \"Header 2\"),\n",
    "    (\"###\", \"Header 3\"),\n",
    "]\n",
    "\n",
    "markdown_splitter = MarkdownHeaderTextSplitter(\n",
    "    headers_to_split_on=headers_to_split_on\n",
    ")\n",
    "\n",
    "chunks_document = markdown_splitter.split_text(documento_markdown)\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=150,\n",
    "    chunk_overlap=20\n",
    ")\n",
    "\n",
    "final_chunks = text_splitter.split_documents(chunks_document)\n",
    "\n",
    "# chunks_document\n",
    "# metadata definidos no headers_to_split_on -> chunk.metadata # {'Header 1': 'Introdução ao PLN'}\n",
    "# conteúdo  # Texto ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. EMBEDDING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definição da função"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modelo BERT é treinado especificamente para a Língua Portuguesa e é bom para tarefas de similaridade semântica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Invalid model-index. Not loading eval results into CardData.\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "ENCODE_MODEL = 'rufimelo/bert-base-portuguese-cased-sts'\n",
    "# ENCODE_MODEL = 'all-MiniLM-L6-v2' # leve, porém melhor para inglês\n",
    "\n",
    "modelo = SentenceTransformer(ENCODE_MODEL)\n",
    "\n",
    "def criar_embeddings(chunks):\n",
    "    \"\"\"Cria embeddings para os chunks\"\"\"\n",
    "    embeddings = modelo.encode(chunks, show_progress_bar=True)\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Execução"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:09<00:00,  9.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensão do embedding:  (7, 1024)\n",
      "[[-0.24512105 -0.2638833  -1.2534713  ...  0.75989485 -1.2004327\n",
      "  -0.34133682]\n",
      " [ 0.68989885  0.05904028 -1.1700327  ...  0.56694835 -0.2709181\n",
      "   0.9166821 ]\n",
      " [ 0.25678015 -0.3784146  -0.59672904 ...  0.5800669  -0.44971\n",
      "   0.29745457]\n",
      " [-0.08029696 -0.29637185 -0.79932034 ...  1.2091796  -0.7239852\n",
      "  -0.0411283 ]\n",
      " [ 0.15125325  0.20062709 -0.46873796 ...  0.02203956 -1.8070877\n",
      "   0.04457791]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "embedding = criar_embeddings(chunks)\n",
    "print(\"Dimensão do embedding: \", embedding.shape)\n",
    "print(embedding[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. VECTOR DATABASE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inicializando cliente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ ChromaDB inicializado: 7 documentos na coleção\n"
     ]
    }
   ],
   "source": [
    "client = chromadb.PersistentClient(path='./chroma_db')\n",
    "\n",
    "collection = client.get_or_create_collection(name='documentos_ml', embedding_function=None)\n",
    "\n",
    "print(f\"✓ ChromaDB inicializado: {collection.count()} documentos na coleção\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adicionar ao chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ 7 chunks adicionados ao ChromaDB\n",
      "Total de documentos na coleção: 7\n"
     ]
    }
   ],
   "source": [
    "# Criar ids únicos\n",
    "ids = [f\"chunk_{i}\" for i in range(len(chunks))]\n",
    "\n",
    "# collection.add(\n",
    "#     ids=ids,\n",
    "#     documents=chunks,\n",
    "#     embeddings=embedding.tolist()  # converter para lista\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. BUSCA VETORIAL (QUERY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pergunta: O que é aprendizado de máquina?\n",
      " - Documento: título introdução machine learning machine learning campo inteligência artificial desenvolver algorit capaz aprender padrão partir dado (Score: 328.4739685058594)\n",
      " - Documento: . rede neural profundo usar visão computacional processamento linguagem natural (Score: 438.04241943359375)\n",
      " - Documento: . modelo cnns transformer ter revolucionar tarefa tradução automático reconhecimento imagem geração texto (Score: 483.20989990234375)\n",
      "\n",
      "Pergunta: Quais são os tipos de aprendizado de máquina?\n",
      " - Documento: título introdução machine learning machine learning campo inteligência artificial desenvolver algorit capaz aprender padrão partir dado (Score: 359.50006103515625)\n",
      " - Documento: . rede neural profundo usar visão computacional processamento linguagem natural (Score: 453.0020446777344)\n",
      " - Documento: . modelo cnns transformer ter revolucionar tarefa tradução automático reconhecimento imagem geração texto (Score: 478.9754333496094)\n"
     ]
    }
   ],
   "source": [
    "questions = [\"O que é aprendizado de máquina?\", \"Quais são os tipos de aprendizado de máquina?\"]\n",
    "\n",
    "for question in questions:\n",
    "    question_embedding = modelo.encode([question])\n",
    "    resultados = collection.query(\n",
    "        query_embeddings=question_embedding.tolist(),\n",
    "        n_results=3\n",
    "    )\n",
    "    print(f\"\\nPergunta: {question}\")\n",
    "    for doc, score in zip(resultados['documents'][0], resultados['distances'][0]):\n",
    "        print(f\" - Documento: {doc} (Score: {score})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
